{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgSPWlved2M7w43o/NILrs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW6tP3-FZq_f"
      },
      "outputs": [],
      "source": [
        "# 1. 모델 정의\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# 모델 정의\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, batch_input_shape=(batch_size, None)),\n",
        "        layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        layers.Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 데이터셋 로드 및 전처리\n",
        "from tensorflow.keras import utils\n",
        "# Shakespeare의 텍스트 데이터를 텐서플로우 데이터셋에서 불러온다.\n",
        "path_to_file = utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# 데이터셋 읽기\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 텍스트의 길이 확인\n",
        "print('Length of text : {} characters'.format(len(text))) # 1115394\n",
        "\n",
        "# 고유 문자 확인\n",
        "vocab = sorted(set(text)) # 65\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHXZK4Nzc6tO",
        "outputId": "ef5afe17-4c6d-45fb-9db0-4b2e0eb7ac6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text : 1115394 characters\n",
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 모델 생성\n",
        "# Vocabulary 길이\n",
        "vocab_size = len(vocab) # 65\n",
        "\n",
        "# 임베딩 차원\n",
        "embedding_dim = 256\n",
        "\n",
        "model = build_model(vocab_size=vocab_size,\n",
        "                    embedding_dim = embedding_dim,\n",
        "                    rnn_units=1024,\n",
        "                    batch_size=64)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUDpNrBUeV2u",
        "outputId": "ac7a3019-07c7-415a-c9fa-03501c5a3300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 문자 인덱싱 및 텐서 변환\n",
        "import numpy as np\n",
        "\n",
        "# 문자 -> 인덱스\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# 인덱스 -> 문자\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "# 텍스트 -> 숫자\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print(text_as_int) # 텍스트를 숫자로 바꾼 데이터 출력해 봄\n",
        "print(text[:3], text[-3:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm4So3LUev8O",
        "outputId": "ede1c544-9927-4502-f38f-680c30bd97f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18 47 56 ... 45  8  0]\n",
            "Fir g.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 시퀀스 및 배치 생성\n",
        "import tensorflow as tf\n",
        "\n",
        "# 시퀀스 길이 정의\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // seq_length # 11153\n",
        "\n",
        "# 데이터셋 생성\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset = dataset.shuffle(examples_per_epoch).batch(64, drop_remainder=True)"
      ],
      "metadata": {
        "id": "A0h0tFzlfOD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 훈련 정의\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "cqNH9RNOfwMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 체크포인트 설정\n",
        "checkpoint_callback= tf.keras.callbacks.ModelCheckpoint(filepath='./checkpoints/ckpt_{epoch}', save_best_only=True)"
      ],
      "metadata": {
        "id": "ebVx4Vzcf5DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. 모델 학습\n",
        "history = model.fit(dataset, epochs=100, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE62NXo0gLPM",
        "outputId": "c32f2709-37c5-44dd-fadb-470a20a16909"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 2.5777"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 1441s 8s/step - loss: 2.5777\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.8842"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 1366s 8s/step - loss: 1.8842\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.6306"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 1364s 8s/step - loss: 1.6306\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.4975"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 1354s 8s/step - loss: 1.4975\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.4175"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 1348s 8s/step - loss: 1.4175\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.3604"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 1352s 8s/step - loss: 1.3604\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.3153"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r172/172 [==============================] - 1354s 8s/step - loss: 1.3153\n",
            "Epoch 8/100\n",
            " 45/172 [======>.......................] - ETA: 16:37 - loss: 1.2768"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. 체크포인트 복원 및 테스트 함수\n",
        "def generate_text(model, start_string, num_generate=300, temperature=1.0):\n",
        "    model_for_inference= build_model(vocab_size, embedding_dim, 1024, batch_size=1) # 추론 모델 정의\n",
        "    model_for_inference.set_weights(model.get_weights()) # 현재 학습된 모델의 파라미터를 추론 모델에 set\n",
        "    model_for_inference.reset_states()\n",
        "    input_data = [char2idx[c] for c in start_string] # 입력 문장을 숫자 리스트로\n",
        "    input_data = tf.expand_dims(input_data, 0)\n",
        "\n",
        "    text_generated = []\n",
        "    for i in range(num_generate):\n",
        "        predictions = model_for_inference(input_data) # 예측 생성\n",
        "        predictions = tf.squeeze(predictions, 0) # 차원을 축소해서 불필요한 차원을 제거\n",
        "        # 예측의 분포를 조절, 온도가 낮으면 더 결정적인 예측, 온도가 높으면 무작위적인 예측\n",
        "        predictions = predictions / temperature\n",
        "        # 샘플링된 다음 문자의 인덱스\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        input_data = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return ''.join(text_generated)"
      ],
      "metadata": {
        "id": "nl0RYOZzgRbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. 체크포인트 복원 및 테스트\n",
        "input_text = input('로미오와 줄리엣 소설의 문장을 입력하세요.')\n",
        "output = generate_text(model, input_text, temperature=1.0)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "9w8IqN9PguRg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}