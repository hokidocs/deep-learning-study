{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70b1895-b204-4683-8035-64065ece86e9",
      "metadata": {
        "id": "c70b1895-b204-4683-8035-64065ece86e9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "resnet_model = ResNet50(input_shape=(224, 224, 3), include_top = False) # 출력층을 제외하고 모델을 가져온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64fe6a3f-e000-4957-bbd7-444c3111ebcf",
      "metadata": {
        "id": "64fe6a3f-e000-4957-bbd7-444c3111ebcf",
        "outputId": "7f128b27-c697-4687-d268-d2487f7fd7de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 100352)            0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               12845184  \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36433283 (138.98 MB)\n",
            "Trainable params: 36380163 (138.78 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "resnet_model.trainable = True\n",
        "model = Sequential([\n",
        "    resnet_model,\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8b8c5b-ad53-4f68-ac10-222480373cd0",
      "metadata": {
        "id": "3d8b8c5b-ad53-4f68-ac10-222480373cd0",
        "outputId": "7698842d-430a-414d-8f43-ce5e4c37a259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1394 images belonging to 3 classes.\n",
            "Found 150 images belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# train 데이터 Image augmentation\n",
        "train_gen = ImageDataGenerator(rotation_range=20, # 회전 각도\n",
        "                               width_shift_range=0.2, # 해당 속성의 값을 정수로 주면 화소의 범위를, 실수를 주면 비율을 단위로 한다.\n",
        "                               height_shift_range=0.2,\n",
        "                               horizontal_flip=True # 좌우반전\n",
        "                              )\n",
        "train_data = train_gen.flow_from_directory('./glaucoma/train', target_size=(224, 224), class_mode='sparse')\n",
        "\n",
        "# test 데이터 Image augmentation\n",
        "test_gen = ImageDataGenerator()\n",
        "test_data = test_gen.flow_from_directory('./glaucoma/test', target_size=(224, 224), class_mode='sparse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cfa92a9-1448-4889-8ce5-faca6fd5a465",
      "metadata": {
        "id": "6cfa92a9-1448-4889-8ce5-faca6fd5a465"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66635137-9a34-4f40-9f90-c06746ece959",
      "metadata": {
        "id": "66635137-9a34-4f40-9f90-c06746ece959",
        "outputId": "4d742eac-65ac-4025-894c-1ae26313ad4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "44/44 [==============================] - 235s 5s/step - loss: 3.3803 - accuracy: 0.6628 - val_loss: 3341002.7500 - val_accuracy: 0.5200\n",
            "Epoch 2/20\n",
            "44/44 [==============================] - 215s 5s/step - loss: 0.7258 - accuracy: 0.7138 - val_loss: 638.2086 - val_accuracy: 0.5200\n",
            "Epoch 3/20\n",
            "44/44 [==============================] - 216s 5s/step - loss: 0.6159 - accuracy: 0.7346 - val_loss: 1.2483 - val_accuracy: 0.7267\n",
            "Epoch 4/20\n",
            "44/44 [==============================] - 216s 5s/step - loss: 0.5291 - accuracy: 0.7489 - val_loss: 2.1814 - val_accuracy: 0.5933\n",
            "Epoch 5/20\n",
            "44/44 [==============================] - 217s 5s/step - loss: 0.5787 - accuracy: 0.7611 - val_loss: 0.9373 - val_accuracy: 0.6800\n",
            "Epoch 6/20\n",
            "44/44 [==============================] - 221s 5s/step - loss: 0.5539 - accuracy: 0.7704 - val_loss: 2.7335 - val_accuracy: 0.5400\n",
            "Epoch 7/20\n",
            "44/44 [==============================] - 217s 5s/step - loss: 0.5097 - accuracy: 0.7798 - val_loss: 1.1003 - val_accuracy: 0.6733\n",
            "Epoch 8/20\n",
            "44/44 [==============================] - 219s 5s/step - loss: 0.5183 - accuracy: 0.7862 - val_loss: 1.3063 - val_accuracy: 0.6733\n",
            "Epoch 9/20\n",
            "44/44 [==============================] - 220s 5s/step - loss: 0.5090 - accuracy: 0.7798 - val_loss: 0.9876 - val_accuracy: 0.6800\n",
            "Epoch 10/20\n",
            "44/44 [==============================] - 218s 5s/step - loss: 0.4599 - accuracy: 0.8006 - val_loss: 0.7991 - val_accuracy: 0.7333\n",
            "Epoch 11/20\n",
            "44/44 [==============================] - 215s 5s/step - loss: 0.4806 - accuracy: 0.7877 - val_loss: 0.8560 - val_accuracy: 0.7000\n",
            "Epoch 12/20\n",
            "44/44 [==============================] - 216s 5s/step - loss: 0.4600 - accuracy: 0.8085 - val_loss: 0.7231 - val_accuracy: 0.6933\n",
            "Epoch 13/20\n",
            "44/44 [==============================] - 216s 5s/step - loss: 0.4634 - accuracy: 0.7927 - val_loss: 1.0596 - val_accuracy: 0.6667\n",
            "Epoch 14/20\n",
            "44/44 [==============================] - 215s 5s/step - loss: 0.4542 - accuracy: 0.7991 - val_loss: 0.5527 - val_accuracy: 0.7200\n",
            "Epoch 15/20\n",
            "44/44 [==============================] - 215s 5s/step - loss: 0.4451 - accuracy: 0.8034 - val_loss: 0.7960 - val_accuracy: 0.7267\n",
            "Epoch 16/20\n",
            "44/44 [==============================] - 221s 5s/step - loss: 0.4680 - accuracy: 0.7934 - val_loss: 0.9925 - val_accuracy: 0.6667\n",
            "Epoch 17/20\n",
            "44/44 [==============================] - 216s 5s/step - loss: 0.4550 - accuracy: 0.7977 - val_loss: 0.6018 - val_accuracy: 0.7267\n",
            "Epoch 18/20\n",
            "44/44 [==============================] - 218s 5s/step - loss: 0.4551 - accuracy: 0.7984 - val_loss: 0.6388 - val_accuracy: 0.7267\n",
            "Epoch 19/20\n",
            "44/44 [==============================] - 220s 5s/step - loss: 0.4225 - accuracy: 0.8293 - val_loss: 0.7331 - val_accuracy: 0.6733\n",
            "Epoch 20/20\n",
            "44/44 [==============================] - 219s 5s/step - loss: 0.4360 - accuracy: 0.8199 - val_loss: 0.6640 - val_accuracy: 0.7267\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2c0c8d6fa50>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_data, validation_data=test_data, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2b56d2-8e4f-407c-bf3a-57b4c75f3df8",
      "metadata": {
        "id": "fd2b56d2-8e4f-407c-bf3a-57b4c75f3df8"
      },
      "outputs": [],
      "source": [
        "model.save('resnet_glaucoma.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "715eb3a5-cf92-49fe-a0e7-35923ecf5054",
      "metadata": {
        "id": "715eb3a5-cf92-49fe-a0e7-35923ecf5054",
        "outputId": "2dca1ab7-6966-4839-81b4-0316d22800e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 914ms/step\n",
            "[0]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "img = image.load_img(\"ad_glaucoma_sample.png\", target_size=(224, 224))\n",
        "x = image.img_to_array(img).reshape(-1, 224, 224, 3)\n",
        "pred = model.predict(x) # 확률값을 가진다.\n",
        "print(np.argmax(pred, axis=1)) # 0:advanced_glaucoma, 1:early_glaucoma, 2:normal_control"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf2.14",
      "language": "python",
      "name": "tf2.14"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}